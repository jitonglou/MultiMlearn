trts, max_size, delta, dist_mat, g_func,
kernel, kpar, C=tuneGrid[i,1], eps),
silent = TRUE
) # return a list with 2 sublists: model and prediction; or "typr-error" class; not every C can yield converged results
## calculate the empirical value function for the test fold
if (class(fit) == "list"){
cv_mat[i,k] = evf(data[which.test,], fit$prediction, propensity[which.test,])
}
}
print(paste("Fold",k,"is done."))
} # end of for (k in 1:nfolds_inner)
cv_est = rowMeans(cv_mat, na.rm = TRUE) # obtain cross-validation estimators
best_idx = which.max(cv_est) # obtain the index of the best tuning parameter(s)
best_param = tuneGrid[best_idx,] # obtain the value(s) of the best tuning parameter(s)
best_param
print("C is selected.")
print(paste0("Best C is 2^", log2(best_param)))
print(paste0("Best C is 2^(", log2(best_param), ")"))
if (is.null(cv_mat) | all(is.na(cv_mat))){
## if no cross-validation result converges, set the final model to NULL
best_fit = NULL
} else {
## get the final model by training on the whole dataset using the best tuning parameter(s)
best_fit = try(
weighted.ksvm.2(
data, idx, which.test=NULL,
trts, max_size, delta, dist_mat, g_func,
kernel, kpar, C=best_param, eps),
silent = TRUE
)
if (class(best_fit) == "try-error") {
best_fit = weighted.ksvm.2(
data=data, idx=idx,
which.test=which(foldid == which.max(cv_mat[best_idx, ])),
trts, max_size, delta, dist_mat, g_func,
kernel, kpar, C=best_param, eps)
}
}
devtools::document()
devtools::document()
data=data_feature
idx=idx_data_feature
trts=levels(data_feature$treatment)
delta=max(dist_feature)
dist_mat=dist_feature
kernel="rbfdot"
kpar="automatic"
tuneGrid=ksvm.grid
propensity=pat_prop
foldid_outer=NULL
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
devtools::document()
data=data_feature
idx=idx_data_feature
trts=levels(data_feature$treatment)
delta=max(dist_feature)
dist_mat=dist_feature
kernel="rbfdot"
kpar="automatic"
tuneGrid=ksvm.grid
propensity=pat_prop
foldid_outer=NULL
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
View(ITR_OVO)
pi_vec = rep(NA, nrow(ITR_OVO$prediction))
for(i in 1:nrow(ITR_OVO$prediction)){
pi_vec[i] = pat_prop[i,ITR_OVO$prediction$treatment[i]]
}
df_evf = data.frame(ITR_OVO$prediction, pi=pi_vec, iter=rseed)
confusionMatrix(as.factor(df_evf$vote), df_evf$treatment)
## EVF of one-size-fits-all rules
df_evf %>%
group_by(treatment) %>%
summarise(evf=sum(reward/pi)/sum(1/pi)) %>%
as.data.frame()
## Different versions of EVF
my_evf = function(reward, pi, x, y){
flag = x==y
return(sum((reward/pi)[flag])/sum((1/pi)[flag]))
}
summary_evf = df_evf %>%
left_join(
simdata %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
group_by(fold) %>%
summarise(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=n()
) %>%
as.data.frame()
summary(summary_evf)
summary_evf = df_evf %>%
left_join(
simdata %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
group_by(fold) %>%
dplyr::summarize(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=n()
) %>%
as.data.frame()
summary_evf = df_evf %>%
dplyr::left_join(
simdata %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
dplyr::group_by(fold) %>%
dplyr::summarize(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=n()
) %>%
as.data.frame()
summary_evf = df_evf %>%
dplyr::left_join(
simdata200 %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
dplyr::group_by(fold) %>%
dplyr::summarize(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=n()
) %>%
as.data.frame()
summary_evf = df_evf %>%
dplyr::left_join(
simdata200 %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
dplyr::group_by(fold) %>%
dplyr::summarize(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=dplyr::n()
) %>%
as.data.frame()
summary(summary_evf)
rm(ITR_OVO)
devtools::document()
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
pracma::combs(K, 2)
devtools::document()
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
devtools::document()
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
data=data_feature
idx=idx_data_feature
trts=levels(data_feature$treatment)
delta=max(dist_feature)
dist_mat=dist_feature
kernel="rbfdot"
kpar="automatic"
tuneGrid=ksvm.grid
propensity=pat_prop
foldid_outer=NULL
tuneGrid
K = length(trts)
## get indices of columns that will be used for prediction
col_select = which(!(colnames(data) %in% c("ID", "cluster", "treatment", "reward", "reward_res")))
fit_list = NULL
data_pred = NULL
nfolds_outer == 1
fit_list = vector("list", nfolds_outer)
foldid_outer = sample(rep(seq(nfolds_outer), length = nrow(data)))
k=1
obj.test = which(foldid_outer == k) # a numeric vector recording index
print(paste("Obj fold",k,"has",length(obj.test),"test samples."))
# data=data[-obj.test,]; idx=idx[-obj.test,]; propensity=propensity[-obj.test,]
fit = weighted.ksvm.2.tune(
data=data[-obj.test,], idx=idx[-obj.test,],
trts, max_size, delta, dist_mat, g_func,
kernel, kpar, eps,
nfolds_inner, tuneGrid, propensity=propensity[-obj.test,])
print("fit is done.")
print(summary(fit))
comb_idx = pracma::combs(seq(K), 2)
sub_num = sum(obj.test)
dim(data)
sub_num = length(obj.test)
pred_mat = mapply(function(model, trt1, trt2){
ifelse(
predict(model, newx = (data[obj.test,] %>%
# select(-ID, -cluster, -treatment, -reward, -reward_res) %>%
select(-(starts_with("ID") | starts_with("cluster") | starts_with("treatment") | starts_with("reward"))) %>%
as.matrix)) >= 0,
trt1, trt2
)},
model = fit$best_fit$model,
trt1 = trts[rep(comb_idx[,1],each=sub_num)] %>% matrix(nrow = sub_num) %>% as.list,
trt2 = trts[rep(comb_idx[,2],each=sub_num)] %>% matrix(nrow =sub_num) %>% as.list
)
trts[rep(comb_idx[,1],each=sub_num)] %>% matrix(nrow=sub_num) %>% as.list
trts[rep(comb_idx[,1],each=sub_num)] %>% matrix(nrow=sub_num)
trts[rep(comb_idx[,1],each=sub_num)] %>% matrix(nrow=sub_num) %>% as.list
trts[rep(comb_idx[,1],each=sub_num)] %>% matrix(nrow=sub_num) %>% list
trts[rep(comb_idx[,1],each=sub_num)] %>% matrix(nrow=sub_num) %>% as.data.frame %>% as.list
devtools::document()
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
pi_vec = rep(NA, nrow(ITR_OVO$prediction))
for(i in 1:nrow(ITR_OVO$prediction)){
pi_vec[i] = pat_prop[i,ITR_OVO$prediction$treatment[i]]
}
df_evf = data.frame(ITR_OVO$prediction, pi=pi_vec, iter=rseed)
#### Print results ####
## Confusion matrix of ITR and assigned treatment
confusionMatrix(as.factor(df_evf$vote), df_evf$treatment)
## EVF of one-size-fits-all rules
df_evf %>%
group_by(treatment) %>%
summarise(evf=sum(reward/pi)/sum(1/pi)) %>%
as.data.frame()
## Different versions of EVF
my_evf = function(reward, pi, x, y){
flag = x==y
return(sum((reward/pi)[flag])/sum((1/pi)[flag]))
}
summary_evf = df_evf %>%
dplyr::left_join(
simdata200 %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
dplyr::group_by(fold) %>%
dplyr::summarize(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=dplyr::n()
) %>%
as.data.frame()
summary(summary_evf)
devtools::document()
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
pi_vec = rep(NA, nrow(ITR_OVO$prediction))
for(i in 1:nrow(ITR_OVO$prediction)){
pi_vec[i] = pat_prop[i,ITR_OVO$prediction$treatment[i]]
}
df_evf = data.frame(ITR_OVO$prediction, pi=pi_vec, iter=rseed)
#### Print results ####
## Confusion matrix of ITR and assigned treatment
confusionMatrix(as.factor(df_evf$vote), df_evf$treatment)
## EVF of one-size-fits-all rules
df_evf %>%
group_by(treatment) %>%
summarise(evf=sum(reward/pi)/sum(1/pi)) %>%
as.data.frame()
## Different versions of EVF
my_evf = function(reward, pi, x, y){
flag = x==y
return(sum((reward/pi)[flag])/sum((1/pi)[flag]))
}
summary_evf = df_evf %>%
dplyr::left_join(
simdata200 %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
dplyr::group_by(fold) %>%
dplyr::summarize(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=dplyr::n()
) %>%
as.data.frame()
summary(summary_evf)
devtools::document()
rm(ITR_OVO)
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
rm(ITR_OVO)
devtools::document()
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
devtools::document()
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
pi_vec = rep(NA, nrow(ITR_OVO$prediction))
for(i in 1:nrow(ITR_OVO$prediction)){
pi_vec[i] = pat_prop[i,ITR_OVO$prediction$treatment[i]]
}
df_evf = data.frame(ITR_OVO$prediction, pi=pi_vec, iter=rseed)
#### Print results ####
## Confusion matrix of ITR and assigned treatment
confusionMatrix(as.factor(df_evf$vote), df_evf$treatment)
## EVF of one-size-fits-all rules
df_evf %>%
group_by(treatment) %>%
summarise(evf=sum(reward/pi)/sum(1/pi)) %>%
as.data.frame()
## Different versions of EVF
my_evf = function(reward, pi, x, y){
flag = x==y
return(sum((reward/pi)[flag])/sum((1/pi)[flag]))
}
summary_evf = df_evf %>%
dplyr::left_join(
simdata200 %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
dplyr::group_by(fold) %>%
dplyr::summarize(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=dplyr::n()
) %>%
as.data.frame()
summary(summary_evf)
cv_mat = matrix(NA,2,2)
cv_est = rowMeans(cv_mat, na.rm = TRUE) # obtain cross-validation estimators
best_idx = which.max(cv_est) # obtain the index of the best tuning parameter(s)
best_param = tuneGrid[best_idx,] # obtain the value(s) of the best tuning parameter(s)
is.nan(best_param)
is.na(best_param)
is.na(cv_est)
devtools::document()
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
pi_vec = rep(NA, nrow(ITR_OVO$prediction))
for(i in 1:nrow(ITR_OVO$prediction)){
pi_vec[i] = pat_prop[i,ITR_OVO$prediction$treatment[i]]
}
df_evf = data.frame(ITR_OVO$prediction, pi=pi_vec, iter=rseed)
#### Print results ####
## Confusion matrix of ITR and assigned treatment
confusionMatrix(as.factor(df_evf$vote), df_evf$treatment)
## EVF of one-size-fits-all rules
df_evf %>%
group_by(treatment) %>%
summarise(evf=sum(reward/pi)/sum(1/pi)) %>%
as.data.frame()
## Different versions of EVF
my_evf = function(reward, pi, x, y){
flag = x==y
return(sum((reward/pi)[flag])/sum((1/pi)[flag]))
}
summary_evf = df_evf %>%
dplyr::left_join(
simdata200 %>% select(ID, pi_true, contains("opt")),
by = "ID") %>%
dplyr::group_by(fold) %>%
dplyr::summarize(
evf_max = my_evf(reward, pi_true, treatment, treatment_opt),
evf_ipw = my_evf(reward, pi, treatment, vote),
evf_ipw_true = my_evf(reward, pi_true, treatment, vote),
evf_ipw_opt = my_evf(reward, pi_opt, treatment_opt, vote),
n=dplyr::n()
) %>%
as.data.frame()
summary(summary_evf)
df_evf %>%
group_by(treatment) %>%
dplyr::summarize(evf=sum(reward/pi)/sum(1/pi)) %>%
as.data.frame()
df_evf %>%
dplyr::group_by(treatment) %>%
dplyr::summarize(evf=sum(reward/pi)/sum(1/pi)) %>%
as.data.frame()
ksvm.grid = expand.grid(C = 2^(-1:1)) # ksvm.grid = expand.grid(C = 2^(-1:1))
nfolds_outer = 2
nfolds_inner = 1 # more folds yield more training samples
eps = 1e-16
g_func = function(x){abs(x)} # weights on the outcome in SVM
max_size = 1 # size of the matched set
set.seed(rseed)
ITR_OVO = weighted.ksvm.2.cv(
data=data_feature, idx=idx_data_feature,
trts=levels(data_feature$treatment), max_size=max_size,
delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
kernel="rbfdot", kpar="automatic", eps=eps,
nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=pat_prop,
foldid_outer=NULL
)
View(fit)
obj.test
sapply(fit$best_fit$model, function(z){
predict(z, newx = (data[obj.test, col_select] %>% as.matrix))
})
ksvm.grid = expand.grid(C = 2^(-1:1)) # ksvm.grid = expand.grid(C = 2^(-1:1))
nfolds_outer = 2
nfolds_inner = 1 # more folds yield more training samples
eps = 1e-16
g_func = function(x){abs(x)} # weights on the outcome in SVM
max_size = 1 # size of the matched set
data=data_feature
idx=idx_data_feature
trts=levels(data_feature$treatment)
delta=max(dist_feature)
dist_mat=dist_feature
kernel="rbfdot"
kpar="automatic"
tuneGrid=ksvm.grid
propensity=pat_prop
foldid_outer=NULL
data(simdata200)
head(simdata200)
ls()
usethat::use_vignette("example")
install.packages("usethat")
devtools::use_vignette("example")
usethis::use_vignette("example")
