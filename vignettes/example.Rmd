---
title: "Vignette of 'MultiMlearn'"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center"
)
```

## Overview
The `MultiMlearn` package is designed for estimating the individualized treatment rules (ITRs) under the setting of multicategory treatments. It is designed for data from observational studies (such as electronic health records), but it can also be used for randomized controlled trials. </br>

The `MultiMlearn` package implements a matched learning (M-learning) model that uses the one-versus-one approach to convert the multicategory treatment comparison to a set of binary classification problems. </br>

Also, the proposed model can incorporate the identified latent subgroup information to alleviate confounding effects among subjects and improve the performance of the M-learning.
The coding example for subgroup identification can be found on [this GitHub repository](https://github.com/jitonglou/IdSubgroup), while it is currently not a function of the `MultiMlearn` package.

## Documentation
A simulation study illustrating the necessity of latent subgroup information can be found in [this supplementary material](https://github.com/jitonglou/MultiMlearn/blob/master/doc/supp_v3.pdf).
The following explainatory example uses a similar simulation scenario to that stated in Section S.1.

## Installation
Install the development version using the `devtools` package:
```{r install, eval=FALSE}
devtools::install_github("jitonglou/MultiMlearn")
```

## Usage
Load the packages:
```{r setup}
library(MultiMlearn)
library(dplyr)
library(caret)
library(personalized)
```

### Simulate a dataset
Following the settings (equation (1), (2), (5), and (6)) in Section S.1.1 and S.1.2 of the [supplementary material](https://github.com/jitonglou/MultiMlearn/blob/master/doc/supp_v3.pdf), we simulate a dataset for N=200 subjects. The covariate vector $\mathbf{x}_i$ was set to be p=20 dimensional. Moreover, we set the number of treatments K=4, the number of groups J=4.
```{r simdata}
rseed = 0
set.seed(rseed)

simdata = simulate_data(
   N = 200, p = 20, K = 4, J = 4,
   propensity_func = pi.true, 
   main_func = mu.true, 
   interaction_func = delta.true
)
# ?simulate_data # explanations of function arguments and the columns in the returned data frame 
# ?pi.true # true propensity model used in the supplementary material
# ?mu.true # true main effects used in the supplementary material
# ?delta.true # true interaction effects used in the supplementary material

table(treatment=simdata$treatment, group=simdata$cluster) # contingency table of observed treatment vs group
```

### Estimate propensity and prognostic scores
Next, we will show how to estimate propensity and prognostic scores using the random forest model. We take subjects in group 1 as an example:
```{r features}
## extract features for subjects in group 1
group = 1
data_covariate = simdata %>%
  filter(cluster == group) %>%
  select(ID, cluster, reward, treatment, starts_with("feature"))
n_feature = ncol(data_covariate) - 4
```

#### Prognostic scores
Train a random forest model to estimate prognostic scores. We selected the tuning parameters (`mtry` and `ntree`) by 10-fold cross-validation with 3 repeats.
```{r prognostic}
# ?rfcv2 # a customized function for using cross-validation to train random forests
## tuning parameters
metric = "RMSE"
tunegrid = expand.grid(.mtry=seq(1,n_feature/3, 1), .ntree=seq(500,2000,500))
control = trainControl(method="repeatedcv", number=10, repeats=3)
## train model (around 1 minute)
set.seed(rseed)
prognostic_fit = train(
  reward~., data=data_covariate %>% select(-ID, -cluster, -treatment),
  method=rfcv2("Regression"), metric=metric,
  tuneGrid=tunegrid, trControl=control
)
print(prognostic_fit)
## estimators
sub_prog = predict(
  prognostic_fit, newdata=data_covariate %>% select(-ID, -cluster, -treatment)
)
```

#### Propensity scores
Train a random forest model to estimate propensity scores. We selected the tuning parameters (`mtry` and `ntree`) by 10-fold cross-validation with 3 repeats.
```{r propensity}
## tuning parameters
metric = "Kappa"
tunegrid = expand.grid(.mtry=seq(1,sqrt(n_feature), 1), .ntree=seq(500,2000,500))
control = trainControl(method="repeatedcv", number=10, repeats=3)
## train model (around 1 minute)
set.seed(rseed)
propensity_fit = train(
  treatment~., data=data_covariate %>% select(-ID, -cluster, -reward),
  method=rfcv2("Classification"), metric=metric,
  tuneGrid=tunegrid, trControl=control
)
print(propensity_fit)
## estimators
sub_prop = predict(
  propensity_fit, newdata=data_covariate %>% select(-ID, -cluster, -reward), type = "prob"
)
```

### Estimate ITRs using the proposed M-learning model
We performs a nested cross-validation (on tuning parameters) of weighted support vector machines (SVMs) to fit the M-learning model and estimate ITRs.

#### Create the final dataset and compute distance between subjects
Incorporate the estimated propensity and prognostics scores to the feature set, and compute the Euclidean distance between subjects based on the features.
```{r final data}
## final data frame
data_feature = data.frame(
  data_covariate %>% mutate(reward_res = reward),
  sub_prop %>% select(-levels(simdata$treatment)[1]), # remove propensity scores of the first treatment to avoid colinearity of propensity scores
  prog = sub_prog
)

## compute the Euclidean distance between features of subjects
dist_feature = data_feature %>%
  select(-ID, -cluster, -treatment, -reward, -reward_res) %>%
  dist() %>%
  as.matrix()
summary(c(dist_feature))

## Create an index data frame to select the corresponding row of the distance matrix
idx_data_feature = data.frame(ID=data_feature$ID, index=1:nrow(data_feature)) 
```

#### Fit the M-learning model
We split the data to 3 folds by letting `nfolds_outer=3`. In each test fold, the tuning parameter in SVMs (cost of constraints violation) is selected from $\{2^k:k=0,\pm1,\ldots,\pm8\}$ using another layer of 3-fold cross-validation (`nfolds_inner=3`). 
```{r mlearning}
## User inputs
ksvm.grid = expand.grid(C = 2^(-8:8)) # a data frame of the tuning parameter
nfolds_outer = 3 # number of folds in the outer layer of the nested cross-validation
nfolds_inner = 3 # number of folds in the inner layer of the nested cross-validation
g_func = function(x){abs(x)} # weights on the outcome in support vector machines
max_size = 1 # maximum size of the matched set

## Fit M-learning model (around 1 minute)
set.seed(rseed)
# ?mlearn.wsvm.cv # explanation of arguments
ITR = mlearn.wsvm.cv(
  data=data_feature, idx=idx_data_feature,
  trts=levels(data_feature$treatment), max_size=max_size,
  delta=max(dist_feature), dist_mat=dist_feature, g_func=g_func,
  kernel="rbfdot", kpar="automatic",
  nfolds_outer=nfolds_outer, nfolds_inner=nfolds_inner, tuneGrid=ksvm.grid, propensity=sub_prop,
  foldid_outer=NULL
)
summary(ITR)
head(ITR$prediction) # "vote" represents the recommended treatments for the subjects. "treatment" represents the observed treatments.
```

### Summarize results
We extract the recommended treatments of the ITR, and then calculate the empirical value function (EVF) and misclassification rate as equation (3) and equation (4) in the [supplementary material](https://github.com/jitonglou/MultiMlearn/blob/master/doc/supp_v3.pdf). 
```{r result}
## create a function to calculate EVF
evf.func = function(reward, pi, x, y){
  flag = (x==y)
  return(sum((reward/pi)[flag])/sum((1/pi)[flag]))
}

## EVF and misclassification rate of observed and recommended treatments
summary_evf = ITR$prediction %>%
  left_join(simdata %>% select(ID, pi_true, contains("opt")), # join the simdata to use true propensity score and optimal treatment information
            by = "ID") %>%
  group_by(fold) %>%
  summarize(
      evf_obs = evf.func(reward, pi_true, treatment, treatment),
      mis_obs = 1-sum(treatment_opt == treatment)/n(),
      evf_rec = evf.func(reward, pi_true, treatment, vote), 
      mis_rec = 1-sum(treatment_opt == vote)/n(),
    n=n() # sample size in each fold
  ) %>%
  as.data.frame

summary(summary_evf)
```
Compared with the observed treatments, the recommended treatments by M-learning have a greater EVF (2.0159 vs 1.1622) and a lower misclassification rate (0 vs. 0.7806) on average. </br> 
To reproduce the results in Table S1 and Figure S1 in Section S.1.3 of the [supplementary material](https://github.com/jitonglou/MultiMlearn/blob/master/doc/supp_v3.pdf), you need to run the above scripts for group 2 to 4, and repeat the procedure for 100 times using `rseed=1,...,100`, `ntree=seq(500,5000,500)`, `ksvm.grid = expand.grid(C = 2^(-15:15))`, `nfolds_outer = 5`, and `nfolds_outer = 5`. Each replication can be done by parallel computing. 

